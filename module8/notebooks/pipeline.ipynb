{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:44.819989Z",
     "start_time": "2021-05-16T14:34:43.555281Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "from ml_metrics import rmsle\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion, make_union\n",
    "from sklearn.base import TransformerMixin, BaseEstimator, RegressorMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, RFE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    quadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return 1.0 - numerator / denominator\n",
    "\n",
    "\n",
    "def linear_weighted_kappa(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Calculates the linear weighted kappa\n",
    "    linear_weighted_kappa calculates the linear weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    linear_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    linear_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = abs(i - j) / float(num_ratings - 1)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return 1.0 - numerator / denominator\n",
    "\n",
    "\n",
    "def kappa(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Calculates the kappa\n",
    "    kappa calculates the kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            if i == j:\n",
    "                d = 0.0\n",
    "            else:\n",
    "                d = 1.0\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return 1.0 - numerator / denominator\n",
    "\n",
    "\n",
    "def mean_quadratic_weighted_kappa(kappas, weights=None):\n",
    "    \"\"\"\n",
    "    Calculates the mean of the quadratic\n",
    "    weighted kappas after applying Fisher's r-to-z transform, which is\n",
    "    approximately a variance-stabilizing transformation.  This\n",
    "    transformation is undefined if one of the kappas is 1.0, so all kappa\n",
    "    values are capped in the range (-0.999, 0.999).  The reverse\n",
    "    transformation is then applied before returning the result.\n",
    "    mean_quadratic_weighted_kappa(kappas), where kappas is a vector of\n",
    "    kappa values\n",
    "    mean_quadratic_weighted_kappa(kappas, weights), where weights is a vector\n",
    "    of weights that is the same size as kappas.  Weights are applied in the\n",
    "    z-space\n",
    "    \"\"\"\n",
    "    kappas = np.array(kappas, dtype=float)\n",
    "    if weights is None:\n",
    "        weights = np.ones(np.shape(kappas))\n",
    "    else:\n",
    "        weights = weights / np.mean(weights)\n",
    "\n",
    "    # ensure that kappas are in the range [-.999, .999]\n",
    "    kappas = np.array([min(x, .999) for x in kappas])\n",
    "    kappas = np.array([max(x, -.999) for x in kappas])\n",
    "\n",
    "    z = 0.5 * np.log((1 + kappas) / (1 - kappas)) * weights\n",
    "    z = np.mean(z)\n",
    "    return (np.exp(2 * z) - 1) / (np.exp(2 * z) + 1)\n",
    "\n",
    "\n",
    "def weighted_mean_quadratic_weighted_kappa(solution, submission):\n",
    "    predicted_score = submission[submission.columns[-1]].copy()\n",
    "    predicted_score.name = \"predicted_score\"\n",
    "    if predicted_score.index[0] == 0:\n",
    "        predicted_score = predicted_score[:len(solution)]\n",
    "        predicted_score.index = solution.index\n",
    "    combined = solution.join(predicted_score, how=\"left\")\n",
    "    groups = combined.groupby(by=\"essay_set\")\n",
    "    kappas = [quadratic_weighted_kappa(group[1][\"essay_score\"], group[1][\"predicted_score\"]) for group in groups]\n",
    "    weights = [group[1][\"essay_weight\"].irow(0) for group in groups]\n",
    "    return mean_quadratic_weighted_kappa(kappas, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae(actual, predicted):\n",
    "    \"\"\"\n",
    "    Computes the absolute error.\n",
    "    This function computes the absolute error between two numbers,\n",
    "    or for element between a pair of lists or numpy arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : int, float, list of numbers, numpy array\n",
    "             The ground truth value\n",
    "    predicted : same type as actual\n",
    "                The predicted value\n",
    "    Returns\n",
    "    -------\n",
    "    score : double or list of doubles\n",
    "            The absolute error between actual and predicted\n",
    "    \"\"\"\n",
    "    return np.abs(np.array(actual)-np.array(predicted))\n",
    "\n",
    "def ce(actual, predicted):\n",
    "    \"\"\"\n",
    "    Computes the classification error.\n",
    "    This function computes the classification error between two lists\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of the true classes\n",
    "    predicted : list\n",
    "                A list of the predicted classes\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The classification error between actual and predicted\n",
    "    \"\"\"\n",
    "    return (sum([1.0 for x,y in zip(actual,predicted) if x != y]) /\n",
    "            len(actual))\n",
    "\n",
    "def mae(actual, predicted):\n",
    "    \"\"\"\n",
    "    Computes the mean absolute error.\n",
    "    This function computes the mean absolute error between two lists\n",
    "    of numbers.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list of numbers, numpy array\n",
    "             The ground truth value\n",
    "    predicted : same type as actual\n",
    "                The predicted value\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean absolute error between actual and predicted\n",
    "    \"\"\"\n",
    "    return np.mean(ae(actual, predicted))\n",
    "\n",
    "def mse(actual, predicted):\n",
    "    \"\"\"\n",
    "    Computes the mean squared error.\n",
    "    This function computes the mean squared error between two lists\n",
    "    of numbers.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list of numbers, numpy array\n",
    "             The ground truth value\n",
    "    predicted : same type as actual\n",
    "                The predicted value\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean squared error between actual and predicted\n",
    "    \"\"\"\n",
    "    return np.mean(se(actual, predicted))\n",
    "\n",
    "def msle(actual, predicted):\n",
    "    \"\"\"\n",
    "    Computes the mean squared log error.\n",
    "    This function computes the mean squared log error between two lists\n",
    "    of numbers.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list of numbers, numpy array\n",
    "             The ground truth value\n",
    "    predicted : same type as actual\n",
    "                The predicted value\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean squared log error between actual and predicted\n",
    "    \"\"\"\n",
    "    return np.mean(sle(actual, predicted))\n",
    "\n",
    "def rmse(actual, predicted):\n",
    "    \"\"\"\n",
    "    Computes the root mean squared error.\n",
    "    This function computes the root mean squared error between two lists\n",
    "    of numbers.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list of numbers, numpy array\n",
    "             The ground truth value\n",
    "    predicted : same type as actual\n",
    "                The predicted value\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The root mean squared error between actual and predicted\n",
    "    \"\"\"\n",
    "    return np.sqrt(mse(actual, predicted))\n",
    "\n",
    "def rmsle(actual, predicted):\n",
    "    \"\"\"\n",
    "    Computes the root mean squared log error.\n",
    "    This function computes the root mean squared log error between two lists\n",
    "    of numbers.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list of numbers, numpy array\n",
    "             The ground truth value\n",
    "    predicted : same type as actual\n",
    "                The predicted value\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The root mean squared log error between actual and predicted\n",
    "    \"\"\"\n",
    "    return np.sqrt(msle(actual, predicted))\n",
    "\n",
    "def se(actual, predicted):\n",
    "    \"\"\"\n",
    "    Computes the squared error.\n",
    "    This function computes the squared error between two numbers,\n",
    "    or for element between a pair of lists or numpy arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : int, float, list of numbers, numpy array\n",
    "             The ground truth value\n",
    "    predicted : same type as actual\n",
    "                The predicted value\n",
    "    Returns\n",
    "    -------\n",
    "    score : double or list of doubles\n",
    "            The squared error between actual and predicted\n",
    "    \"\"\"\n",
    "    return np.power(np.array(actual)-np.array(predicted), 2)\n",
    "\n",
    "def sle(actual, predicted):\n",
    "    \"\"\"\n",
    "    Computes the squared log error.\n",
    "    This function computes the squared log error between two numbers,\n",
    "    or for element between a pair of lists or numpy arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : int, float, list of numbers, numpy array\n",
    "             The ground truth value\n",
    "    predicted : same type as actual\n",
    "                The predicted value\n",
    "    Returns\n",
    "    -------\n",
    "    score : double or list of doubles\n",
    "            The squared log error between actual and predicted\n",
    "    \"\"\"\n",
    "    return (np.power(np.log(np.array(actual)+1) - \n",
    "            np.log(np.array(predicted)+1), 2))\n",
    "\n",
    "def ll(actual, predicted):\n",
    "    \"\"\"\n",
    "    Computes the log likelihood.\n",
    "    This function computes the log likelihood between two numbers,\n",
    "    or for element between a pair of lists or numpy arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : int, float, list of numbers, numpy array\n",
    "             The ground truth value\n",
    "    predicted : same type as actual\n",
    "                The predicted value\n",
    "    Returns\n",
    "    -------\n",
    "    score : double or list of doubles\n",
    "            The log likelihood error between actual and predicted\n",
    "    \"\"\"\n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted)\n",
    "    err = np.seterr(all='ignore')\n",
    "    score = -(actual*np.log(predicted)+(1-actual)*np.log(1-predicted))\n",
    "    np.seterr(divide=err['divide'], over=err['over'],\n",
    "              under=err['under'], invalid=err['invalid'])\n",
    "    if type(score)==np.ndarray:\n",
    "        score[np.isnan(score)] = 0\n",
    "    else:\n",
    "        if np.isnan(score):\n",
    "            score = 0\n",
    "    return score\n",
    "\n",
    "def log_loss(actual, predicted):\n",
    "    \"\"\"\n",
    "    Computes the log loss.\n",
    "    This function computes the log loss between two lists\n",
    "    of numbers.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list of numbers, numpy array\n",
    "             The ground truth value\n",
    "    predicted : same type as actual\n",
    "                The predicted value\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The log loss between actual and predicted\n",
    "    \"\"\"\n",
    "    return np.mean(ll(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:45.078739Z",
     "start_time": "2021-05-16T14:34:44.856216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10886 entries, 0 to 10885\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   datetime    10886 non-null  object \n",
      " 1   season      10886 non-null  int64  \n",
      " 2   holiday     10886 non-null  int64  \n",
      " 3   workingday  10886 non-null  int64  \n",
      " 4   weather     10886 non-null  int64  \n",
      " 5   temp        10886 non-null  float64\n",
      " 6   atemp       10886 non-null  float64\n",
      " 7   humidity    10886 non-null  int64  \n",
      " 8   windspeed   10886 non-null  float64\n",
      " 9   casual      10886 non-null  int64  \n",
      " 10  registered  10886 non-null  int64  \n",
      " 11  count       10886 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_hdf('../input/train_bike.h5')\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dane\n",
    "Wróćmy do zbioru, którego dotknęliśmy w module 4 - `Bike Sharing Demand`.\n",
    "\n",
    "Zadaniem jest zrobienie predykcji popytu - ile rowerów powinno być na danej stacji dla danej godziny.\n",
    "\n",
    "https://www.kaggle.com/c/bike-sharing-demand/data\n",
    "\n",
    "- **datetime** - hourly date + timestamp  \n",
    "- **season** -  1 = spring, 2 = summer, 3 = fall, 4 = winter \n",
    "- **holiday** - whether the day is considered a holiday\n",
    "- **workingday** - whether the day is neither a weekend nor holiday\n",
    "- **weather** -   \n",
    "    1: Clear, Few clouds, Partly cloudy, Partly cloudy  \n",
    "    2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist  \n",
    "    3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds  \n",
    "    4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
    "- **temp** - temperature in Celsius\n",
    "- **atemp** - \"feels like\" temperature in Celsius\n",
    "- **humidity** - relative humidity\n",
    "- **windspeed** - wind speed\n",
    "- **casual** - number of non-registered user rentals initiated\n",
    "- **registered** - number of registered user rentals initiated\n",
    "- **count** - number of total rentals\n",
    "\n",
    "*Zwróć uwagę*, że `count = casual + registered`. To oznacza, że trzy kolumny są powiązane ze zmienną docelową i nie mogą być użyte podczas trenowania.\n",
    "\n",
    "Sprawdźmy rozkład zmiennej docelowej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:45.612499Z",
     "start_time": "2021-05-16T14:34:45.118092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3MAAAEvCAYAAADvmpjfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAZBUlEQVR4nO3df6xn5V0n8PdHRrEtqaXBzrIzZIdNJiqUdbUTFm1iLosb2MVI/2Ezhiq6bMgarF1DouD+0b9ISHZrrFGaTEpXmjadRewGYq1KcO8akhYs1iwFZDspLIxg0bXt9rIbdPCzf9yjuU7vMDP3e++589z7eiU333Oe8+v5wucyvOd5zjnV3QEAAGAs37LdHQAAAODsCXMAAAADEuYAAAAGJMwBAAAMSJgDAAAYkDAHAAAwoD3b3YHTueiii/rAgQPb3Y2/59VXX81b3vKW7e4Gu4R6Y07qjTmpN+ak3pjTZtfbE0888Rfd/Z0nt5/zYe7AgQP5/Oc/v93d+HuWl5eztLS03d1gl1BvzEm9MSf1xpzUG3Pa7Hqrqv+1XrtplgAAAAMS5gAAAAYkzAEAAAxImAMAABiQMAcAADAgYQ4AAGBAwhwAAMCAhDkAAIABCXMAAAADEuYAAAAGJMwBAAAMaM92d2BUB+749Cm3PX/39TP2BAAA2I2MzAEAAAzotGGuqj5aVa9U1RfXtP3HqvqTqvofVfVfq+pta7bdWVXHqurZqrp2Tfu7qurJaduvVFVt+rcBAADYJc5kZO7Xk1x3UtvDSd7Z3f8kyf9McmeSVNVlSQ4nuXw65p6qOm865sNJbk1ycPo5+ZwAAACcodOGue7+gyR/eVLb73X3iWn1c0n2T8s3JDna3a9193NJjiW5sqouTvLW7v5sd3eSjyV5zyZ9BwAAgF1nM+6Z+zdJPjMt70vy4pptx6e2fdPyye0AAABswEJPs6yq/5DkRJJP/G3TOrv1G7Sf6ry3ZnVKZvbu3Zvl5eVFurnpVlZWcvsVr59y+7nWX8a2srKippiNemNO6o05qTfmNFe9bTjMVdXNSX4kyTXT1MlkdcTtkjW77U/y0tS+f532dXX3kSRHkuTQoUO9tLS00W5uieXl5Xzw0VdPuf35m5bm6ww73vLycs613wF2LvXGnNQbc1JvzGmuetvQNMuqui7JLyT50e7+v2s2PZTkcFWdX1WXZvVBJ49398tJvlFVV01PsfyJJA8u2HcAAIBd67Qjc1X1ySRLSS6qquNJPpDVp1een+Th6Q0Dn+vuf9fdT1XV/Umezur0y9u6+2/nI/50Vp+M+aas3mP3mQAAALAhpw1z3f1j6zTf+wb735XkrnXaP5/knWfVOwAAANa1GU+zBAAAYGbCHAAAwICEOQAAgAEJcwAAAAMS5gAAAAYkzAEAAAxImAMAABiQMAcAADAgYQ4AAGBAwhwAAMCAhDkAAIABCXMAAAADEuYAAAAGJMwBAAAMSJgDAAAYkDAHAAAwIGEOAABgQMIcAADAgIQ5AACAAQlzAAAAAxLmAAAABiTMAQAADEiYAwAAGJAwBwAAMCBhDgAAYEDCHAAAwICEOQAAgAEJcwAAAAMS5gAAAAYkzAEAAAxImAMAABiQMAcAADAgYQ4AAGBApw1zVfXRqnqlqr64pu3tVfVwVX1p+rxwzbY7q+pYVT1bVdeuaX9XVT05bfuVqqrN/zoAAAC7w5mMzP16kutOarsjySPdfTDJI9N6quqyJIeTXD4dc09VnTcd8+EktyY5OP2cfE4AAADO0GnDXHf/QZK/PKn5hiT3Tcv3JXnPmvaj3f1adz+X5FiSK6vq4iRv7e7Pdncn+diaYwAAADhLG71nbm93v5wk0+c7pvZ9SV5cs9/xqW3ftHxyOwAAABuwZ5PPt959cP0G7eufpOrWrE7JzN69e7O8vLwpndssKysruf2K10+5/VzrL2NbWVlRU8xGvTEn9cac1BtzmqveNhrmvlJVF3f3y9MUylem9uNJLlmz3/4kL03t+9dpX1d3H0lyJEkOHTrUS0tLG+zm1lheXs4HH331lNufv2lpvs6w4y0vL+dc+x1g51JvzEm9MSf1xpzmqreNTrN8KMnN0/LNSR5c0364qs6vqkuz+qCTx6epmN+oqqump1j+xJpjAAAAOEunHZmrqk8mWUpyUVUdT/KBJHcnub+qbknyQpIbk6S7n6qq+5M8neREktu6+2/nI/50Vp+M+aYkn5l+AAAA2IDThrnu/rFTbLrmFPvfleSuddo/n+SdZ9U7AAAA1rXRaZYAAABsI2EOAABgQMIcAADAgIQ5AACAAQlzAAAAAxLmAAAABiTMAQAADEiYAwAAGJAwBwAAMCBhDgAAYEDCHAAAwICEOQAAgAEJcwAAAAMS5gAAAAYkzAEAAAxImAMAABiQMAcAADAgYQ4AAGBAwhwAAMCAhDkAAIABCXMAAAADEuYAAAAGJMwBAAAMSJgDAAAYkDAHAAAwIGEOAABgQMIcAADAgIQ5AACAAQlzAAAAAxLmAAAABiTMAQAADEiYAwAAGJAwBwAAMCBhDgAAYEALhbmq+rmqeqqqvlhVn6yqb6+qt1fVw1X1penzwjX731lVx6rq2aq6dvHuAwAA7E4bDnNVtS/JzyY51N3vTHJeksNJ7kjySHcfTPLItJ6qumzafnmS65LcU1XnLdZ9AACA3WnRaZZ7krypqvYkeXOSl5LckOS+aft9Sd4zLd+Q5Gh3v9bdzyU5luTKBa8PAACwK1V3b/zgqvcnuSvJ/0vye919U1V9rbvftmafr3b3hVX1q0k+190fn9rvTfKZ7n5gnfPemuTWJNm7d++7jh49uuE+boWVlZU89/XXT7n9in3fMWNv2OlWVlZywQUXbHc32CXUG3NSb8xJvTGnza63q6+++onuPnRy+56NnnC6F+6GJJcm+VqS36iq977RIeu0rZsku/tIkiNJcujQoV5aWtpoN7fE8vJyPvjoq6fc/vxNS/N1hh1veXk559rvADuXemNO6o05qTfmNFe9LTLN8oeTPNfdf97df53kU0l+MMlXquriJJk+X5n2P57kkjXH78/qtEwAAADO0iJh7oUkV1XVm6uqklyT5JkkDyW5edrn5iQPTssPJTlcVedX1aVJDiZ5fIHrAwAA7FobnmbZ3Y9V1QNJ/ijJiSRfyOrUyAuS3F9Vt2Q18N047f9UVd2f5Olp/9u6+9Q3ngEAAHBKGw5zSdLdH0jygZOaX8vqKN16+9+V1QemAAAAsIBFX00AAADANhDmAAAABiTMAQAADEiYAwAAGJAwBwAAMCBhDgAAYEDCHAAAwICEOQAAgAEJcwAAAAMS5gAAAAYkzAEAAAxImAMAABiQMAcAADAgYQ4AAGBAwhwAAMCAhDkAAIABCXMAAAADEuYAAAAGJMwBAAAMSJgDAAAYkDAHAAAwIGEOAABgQMIcAADAgIQ5AACAAQlzAAAAAxLmAAAABiTMAQAADEiYAwAAGJAwBwAAMCBhDgAAYEB7trsDO9GBOz79htufv/v6mXoCAADsVEbmAAAABiTMAQAADGihMFdVb6uqB6rqT6rqmar6gap6e1U9XFVfmj4vXLP/nVV1rKqeraprF+8+AADA7rToyNyHkvxOd393ku9N8kySO5I80t0HkzwyraeqLktyOMnlSa5Lck9Vnbfg9QEAAHalDYe5qnprkh9Kcm+SdPdfdffXktyQ5L5pt/uSvGdaviHJ0e5+rbufS3IsyZUbvT4AAMBuVt29sQOr/mmSI0mezuqo3BNJ3p/kT7v7bWv2+2p3X1hVv5rkc9398an93iSf6e4H1jn3rUluTZK9e/e+6+jRoxvq41ZZWVnJc19/fcPHX7HvOzaxN+x0KysrueCCC7a7G+wS6o05qTfmpN6Y02bX29VXX/1Edx86uX2RVxPsSfL9Sd7X3Y9V1YcyTak8hVqnbd0k2d1HshoUc+jQoV5aWlqgm5tveXk5H3z01Q0f//xNS5vXGXa85eXlnGu/A+xc6o05qTfmpN6Y01z1tsg9c8eTHO/ux6b1B7Ia7r5SVRcnyfT5ypr9L1lz/P4kLy1wfQAAgF1rw2Guu/8syYtV9V1T0zVZnXL5UJKbp7abkzw4LT+U5HBVnV9VlyY5mOTxjV4fAABgN1tkmmWSvC/JJ6rq25J8OclPZTUg3l9VtyR5IcmNSdLdT1XV/VkNfCeS3NbdG7/xDAAAYBdbKMx19x8n+aYb8bI6Srfe/ncluWuRawIAALD4e+YAAADYBsIcAADAgIQ5AACAAQlzAAAAAxLmAAAABiTMAQAADEiYAwAAGJAwBwAAMCBhDgAAYEDCHAAAwICEOQAAgAEJcwAAAAMS5gAAAAYkzAEAAAxImAMAABiQMAcAADAgYQ4AAGBAwhwAAMCAhDkAAIABCXMAAAADEuYAAAAGJMwBAAAMSJgDAAAYkDAHAAAwIGEOAABgQMIcAADAgIQ5AACAAe3Z7g7sRgfu+PQbbn/+7utn6gkAADAqI3MAAAADEuYAAAAGJMwBAAAMSJgDAAAYkDAHAAAwoIXDXFWdV1VfqKrfmtbfXlUPV9WXps8L1+x7Z1Udq6pnq+raRa8NAACwW23GyNz7kzyzZv2OJI9098Ekj0zrqarLkhxOcnmS65LcU1XnbcL1AQAAdp2FwlxV7U9yfZKPrGm+Icl90/J9Sd6zpv1od7/W3c8lOZbkykWuDwAAsFstOjL3y0l+PsnfrGnb290vJ8n0+Y6pfV+SF9fsd3xqAwAA4Czt2eiBVfUjSV7p7ieqaulMDlmnrU9x7luT3Joke/fuzfLy8gZ7uTVWVlZy+xWvb9n5z7Xvy/ZaWVlRE8xGvTEn9cac1BtzmqveNhzmkrw7yY9W1b9K8u1J3lpVH0/ylaq6uLtfrqqLk7wy7X88ySVrjt+f5KX1TtzdR5IcSZJDhw710tLSAt3cfMvLy/ngo69u2fmfv2lpy87NeJaXl3Ou/Q6wc6k35qTemJN6Y05z1duGp1l2953dvb+7D2T1wSa/393vTfJQkpun3W5O8uC0/FCSw1V1flVdmuRgksc33HMAAIBdbJGRuVO5O8n9VXVLkheS3Jgk3f1UVd2f5OkkJ5Lc1t1bN1cRAABgB9uUMNfdy0mWp+X/neSaU+x3V5K7NuOaAAAAu9lmvGcOAACAmQlzAAAAAxLmAAAABrQVD0BhQQfu+PQptz1/9/Uz9gQAADhXGZkDAAAYkDAHAAAwIGEOAABgQMIcAADAgIQ5AACAAQlzAAAAAxLmAAAABiTMAQAADEiYAwAAGJAwBwAAMCBhDgAAYEDCHAAAwICEOQAAgAHt2e4OcO44cMen33D783dfP1NPAACA0xHmdpg3CmTCGAAA7BymWQIAAAxImAMAABiQaZa7yOnuiQMAAMYhzA3mXA1kHp4CAADzMs0SAABgQMIcAADAgIQ5AACAAblnjlkscq/fVt5v5718AACMSpjjnOfhKgAA8M2EOdggIRMAgO0kzDE8UyUBANiNhDnYIkImAABbSZhjRztXX7IOAACL8moCAACAAW14ZK6qLknysST/IMnfJDnS3R+qqrcn+S9JDiR5Psm/7u6vTsfcmeSWJK8n+dnu/t2Fes+sjHIBAMC5Y5GRuRNJbu/u70lyVZLbquqyJHckeaS7DyZ5ZFrPtO1wksuTXJfknqo6b5HOAwAA7FYbHpnr7peTvDwtf6OqnkmyL8kNSZam3e5LspzkF6b2o939WpLnqupYkiuTfHajfYBRea0BAACL2pQHoFTVgSTfl+SxJHunoJfufrmq3jHtti/J59YcdnxqA87SIlNeBUUAgJ2hunuxE1RdkOS/J7mruz9VVV/r7ret2f7V7r6wqn4tyWe7++NT+71Jfru7f3Odc96a5NYk2bt377uOHj26UB8328rKSp77+uvb3Q222BX7vuMNtz/5p1+f5dorKyu54IILNu3ap/te7G7r1RtsFfXGnNQbc9rserv66quf6O5DJ7cvNDJXVd+a5DeTfKK7PzU1f6WqLp5G5S5O8srUfjzJJWsO35/kpfXO291HkhxJkkOHDvXS0tIi3dx0y8vL+eCjr253N9hqT57u3/HWvdnj+ZuW/m55eXk5J/8O/OQiD6N5g+9l1I716g22inpjTuqNOc1Vbxt+AEpVVZJ7kzzT3b+0ZtNDSW6elm9O8uCa9sNVdX5VXZrkYJLHN3p9AACA3WyRoYV3J/nxJE9W1R9Pbb+Y5O4k91fVLUleSHJjknT3U1V1f5Kns/okzNu621xFWMfae+Juv+LEYiNxAADsSIs8zfLRJHWKzdec4pi7kty10WsCAACwapH3zAEAALBNtu4JDgBreLceAMDmMjIHAAAwIGEOAABgQKZZApvmdFMpN3qsKZgAAN/MyBwAAMCAhDkAAIABmWYJ/J1FpkkCADAvI3MAAAADMjIHnPO8ow4A4JsJc8DwhD0AYDcS5oAdz2sPAICdyD1zAAAAAzIyB+xqizzBc9FRPSOGAMAijMwBAAAMSJgDAAAYkGmWABt0Lr9k3RROANj5hDmAAZ3LQRIAmIcwB3AOEtYAgNNxzxwAAMCAjMwBcMZON2LofjwAmI8wB7DLvFEgu/2KE/lJUzwBYAjCHABD8IROAPj7hDkANs1OnIa5E78TADuDMAfA8AQuAHYjYQ6A2ezEVy6Y/gnAdhHmANjxdmKIXISRTICdQZgDgC0iNAGwlYQ5ABiQoAiAMAcA22TUQOY+QYBzgzAHADvQIvcJbuWxwh7A5hHmAIAd4VwdMTxX+wWMT5gDgHPUTnwK59rvdPsVJ/KTJ33HNwo32zXamGxdvwAWMXuYq6rrknwoyXlJPtLdd8/dBwDg3LTbgtGi01KN+sHuNmuYq6rzkvxakn+R5HiSP6yqh7r76Tn7AQBwNnZiyFwkCLo38psJ1myHuUfmrkxyrLu/nCRVdTTJDUmEOQCAk5yrD7LZyuO3akrretN657KVfxkgKO5uc4e5fUleXLN+PMk/m7kPAACco3biKOhW2q5/XqOGyJ02qlzdPd/Fqm5Mcm13/9tp/ceTXNnd7ztpv1uT3DqtfleSZ2fr5Jm5KMlfbHcn2DXUG3NSb8xJvTEn9cacNrve/lF3f+fJjXOPzB1Pcsma9f1JXjp5p+4+kuTIXJ06W1X1+e4+tN39YHdQb8xJvTEn9cac1BtzmqvevmWrL3CSP0xysKourapvS3I4yUMz9wEAAGB4s47MdfeJqvqZJL+b1VcTfLS7n5qzDwAAADvB7O+Z6+7fTvLbc193k52zU0DZkdQbc1JvzEm9MSf1xpxmqbdZH4ACAADA5pj7njkAAAA2gTB3Fqrquqp6tqqOVdUd290fxldVl1TVf6uqZ6rqqap6/9T+9qp6uKq+NH1euOaYO6cafLaqrt2+3jOqqjqvqr5QVb81ras3tkRVva2qHqiqP5n+O/cD6o2tUlU/N/1Z+sWq+mRVfbt6Y7NU1Uer6pWq+uKatrOur6p6V1U9OW37laqqRfolzJ2hqjovya8l+ZdJLkvyY1V12fb2ih3gRJLbu/t7klyV5Lapru5I8kh3H0zyyLSeadvhJJcnuS7JPVNtwtl4f5Jn1qyrN7bKh5L8Tnd/d5LvzWrdqTc2XVXtS/KzSQ519zuz+qC9w1FvbJ5fz2qtrLWR+vpwVt+nfXD6OfmcZ0WYO3NXJjnW3V/u7r9KcjTJDdvcJwbX3S939x9Ny9/I6v/o7Mtqbd037XZfkvdMyzckOdrdr3X3c0mOZbU24YxU1f4k1yf5yJpm9camq6q3JvmhJPcmSXf/VXd/LeqNrbMnyZuqak+SN2f1XcbqjU3R3X+Q5C9Paj6r+qqqi5O8tbs/26sPLvnYmmM2RJg7c/uSvLhm/fjUBpuiqg4k+b4kjyXZ290vJ6uBL8k7pt3UIYv65SQ/n+Rv1rSpN7bCP07y50n+8zSt9yNV9ZaoN7ZAd/9pkv+U5IUkLyf5enf/XtQbW+ts62vftHxy+4YJc2duvfmsHgXKpqiqC5L8ZpJ/393/5412XadNHXJGqupHkrzS3U+c6SHrtKk3ztSeJN+f5MPd/X1JXs00BekU1BsbNt2rdEOSS5P8wyRvqar3vtEh67SpNzbLqepr0+tOmDtzx5NcsmZ9f1aH72EhVfWtWQ1yn+juT03NX5mG4jN9vjK1q0MW8e4kP1pVz2d1qvg/r6qPR72xNY4nOd7dj03rD2Q13Kk3tsIPJ3muu/+8u/86yaeS/GDUG1vrbOvr+LR8cvuGCXNn7g+THKyqS6vq27J6U+ND29wnBjc9wejeJM909y+t2fRQkpun5ZuTPLim/XBVnV9Vl2b1xtnH5+ovY+vuO7t7f3cfyOp/w36/u98b9cYW6O4/S/JiVX3X1HRNkqej3tgaLyS5qqrePP3Zek1W70NXb2yls6qvaSrmN6rqqqlOf2LNMRuyZ5GDd5PuPlFVP5Pkd7P6hKSPdvdT29wtxvfuJD+e5Mmq+uOp7ReT3J3k/qq6Jat/QN2YJN39VFXdn9X/ITqR5Lbufn32XrPTqDe2yvuSfGL6S9AvJ/mprP5FsnpjU3X3Y1X1QJI/ymr9fCHJkSQXRL2xCarqk0mWklxUVceTfCAb+/Pzp7P6ZMw3JfnM9LPxfq0+SAUAAICRmGYJAAAwIGEOAABgQMIcAADAgIQ5AACAAQlzAAAAAxLmAAAABiTMAQAADEiYAwAAGND/B1Yya9129OFjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train['count'].hist(bins=100, figsize=(15, 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Swoją drogą*, czy to dla Ciebie była niespodzianka, że rozkład z \"popytem\" będzie wyglądał na skrzywiony? \n",
    "Pewnie już wiesz, co chcemy z tym zrobić, ale na razie zostawmy tak jak jest.\n",
    "\n",
    "Metryką sukcesu będzie **RMSLE** (która bardziej karze zaniżanie wyniku). W ramach przypomnienia poniżej wzór:\n",
    "\n",
    "$$ \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2 }$$\n",
    "\n",
    "Zbudujmy najprostszy możliwy model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:45.668830Z",
     "start_time": "2021-05-16T14:34:45.653460Z"
    }
   },
   "outputs": [],
   "source": [
    "feats = df_train.select_dtypes(include=[np.int64, np.float64]).columns\n",
    "feats = feats[ (feats != 'count') & (feats != 'registered') & (feats != 'casual') ]\n",
    "\n",
    "X = df_train[feats].values\n",
    "y = df_train['count'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:45.716452Z",
     "start_time": "2021-05-16T14:34:45.705313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy-mean 1.5691983019475926\n",
      "dummy-median 1.4725894242962372\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    ('dummy-mean', DummyRegressor(strategy='mean')),\n",
    "    ('dummy-median', DummyRegressor(strategy='median')),\n",
    "]\n",
    "\n",
    "for model_name, model in models:\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    score = rmsle(y, y_pred)\n",
    "    \n",
    "    print(model_name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz spróbujmy użyć drzewa decyzyjnego i lasów losowych.\n",
    "\n",
    "*Zwróć uwagę*, że podaję `random_state`, żeby wynik był powtarzalny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:46.007173Z",
     "start_time": "2021-05-16T14:34:45.758522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt 1.3345865242240684\n",
      "rf 1.3325396672694263\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    ('dt', DecisionTreeRegressor(max_depth=5, random_state=2019)),\n",
    "    ('rf', RandomForestRegressor(max_depth=5, n_estimators=20, random_state=2019)),\n",
    "]\n",
    "\n",
    "for model_name, model in models:\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    score = rmsle(y, y_pred)\n",
    "    \n",
    "    print(model_name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wynik jest trochę lepszy niż dla `dummy`, ale jest jeszcze nad czym popracować. W tej chwili w ogóle nie wykorzystujemy cechy `datetime`, więc coś z tym zaraz zrobimy.\n",
    "\n",
    "Warto już również zacząć myśleć o tym, jak będziemy walidować rozwiązanie, bo cały czas trenujemy i testujemy na tym samym zbiorze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:46.065714Z",
     "start_time": "2021-05-16T14:34:46.043792Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train['datetime'] = pd.to_datetime( df_train['datetime'] )\n",
    "\n",
    "df_train['hour'] = df_train['datetime'].dt.hour\n",
    "df_train['day'] = df_train['datetime'].dt.day\n",
    "df_train['month'] = df_train['datetime'].dt.month\n",
    "df_train['year'] = df_train['datetime'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pojawiły się nowe cechy, więc trzeba jeszcze raz wczytać dane do `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:46.124320Z",
     "start_time": "2021-05-16T14:34:46.110544Z"
    }
   },
   "outputs": [],
   "source": [
    "feats = df_train.select_dtypes(include=[np.int64, np.float64]).columns\n",
    "feats = feats[ (feats != 'count') & (feats != 'registered') & (feats != 'casual') ]\n",
    "\n",
    "X = df_train[feats].values\n",
    "y = df_train['count'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:46.501270Z",
     "start_time": "2021-05-16T14:34:46.163325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt 0.6021408888997963\n",
      "rf 0.5949945087320397\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    ('dt', DecisionTreeRegressor(max_depth=5, random_state=2019)),\n",
    "    ('rf', RandomForestRegressor(max_depth=5, n_estimators=20, random_state=2019)),\n",
    "]\n",
    "\n",
    "for model_name, model in models:\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    score = rmsle(y, y_pred)\n",
    "    \n",
    "    print(model_name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:46.551403Z",
     "start_time": "2021-05-16T14:34:46.545246Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_validation(data, feats, target_variable='count', n_folds=3):\n",
    "    X = data[feats].values\n",
    "    y = data[target_variable].values\n",
    "\n",
    "    groups = data['datetime'].dt.month.values\n",
    "    group_kfold = GroupKFold(n_splits=n_folds)\n",
    "    \n",
    "    for train_idx, test_idx in group_kfold.split(X, y, groups):\n",
    "        yield X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "        \n",
    "        \n",
    "def run_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return rmsle(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:47.245152Z",
     "start_time": "2021-05-16T14:34:46.585054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: dt, scores-mean: 0.6209599748149964, scores-std: 0.012074654497161295\n",
      "Model: rf, scores-mean: 0.6045477718222559, scores-std: 0.013652588538452191\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models:\n",
    "    scores = []\n",
    "    for X_train, X_test, y_train, y_test in custom_validation(df_train, feats):\n",
    "        score = run_model(model, X_train, X_test, y_train, y_test)\n",
    "        scores.append(score)\n",
    "        \n",
    "    print(\"Model: {0}, scores-mean: {1}, scores-std: {2}\".format(model_name, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "Spróbujmy wygenerować jeszcze więcej cech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:47.298809Z",
     "start_time": "2021-05-16T14:34:47.280086Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/00/3f1t2rhd45l6fvmbkz20ptgh00dr78/T/ipykernel_9436/1917838673.py:2: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n",
      "  df_train['weekofyear'] = df_train['datetime'].dt.weekofyear\n"
     ]
    }
   ],
   "source": [
    "df_train['dayofweek'] = df_train['datetime'].dt.dayofweek\n",
    "df_train['weekofyear'] = df_train['datetime'].dt.weekofyear\n",
    "df_train['weekend'] = df_train.dayofweek.map(lambda x: int(x in [5,6]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:47.343460Z",
     "start_time": "2021-05-16T14:34:47.329629Z"
    }
   },
   "outputs": [],
   "source": [
    "feats = df_train.select_dtypes(include=[np.int64, np.float64]).columns\n",
    "feats = feats[ (feats != 'count') & (feats != 'registered') & (feats != 'casual') ]\n",
    "\n",
    "X = df_train[feats].values\n",
    "y = df_train['count'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:48.128662Z",
     "start_time": "2021-05-16T14:34:47.375478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: dt, scores-mean: 0.6295266771184037, scores-std: 0.0162794568121357\n",
      "Model: rf, scores-mean: 0.608049914962654, scores-std: 0.017740841432186828\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models:\n",
    "    scores = []\n",
    "    for X_train, X_test, y_train, y_test in custom_validation(df_train, feats):\n",
    "        score = run_model(model, X_train, X_test, y_train, y_test)\n",
    "        scores.append(score)\n",
    "        \n",
    "    print(\"Model: {0}, scores-mean: {1}, scores-std: {2}\".format(model_name, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widać, że wynik jest porównywalny a nawet trochę gorszy... Również widać, że część kodu powtarza się. Mało tego, bardzo łatwo można się pogubić i odnotować, które cechy są stworzone i w którym miejscu. Dodatkowo, pamiętaj że dokładnie te same transformacje trzeba zrobić zarówno dla zbioru treningowego, jak i testowego.\n",
    "\n",
    "Spróbujmy to uporządkować.\n",
    "\n",
    "Możesz zauważyć, że przedtem zrobiliśmy kilka kroków:\n",
    "1. Wczytanie danych.\n",
    "2. Transformacja (wybranie odpowiednich kolumn i generowanie nowych).\n",
    "3. Budowanie modelu.\n",
    "\n",
    "To wszystko możemy podłączyć do `Pipeline`, które oczekuje na kroki działania. Warto teraz wspomnieć o dwóch rodzajach klas w `sklearn`:\n",
    "- **Transformers** (posiada metody: `fit` i `transform`)\n",
    "- **Estimators** (posiada metody: `fit` i `predict`)\n",
    "\n",
    "`Pipeline` może zawierać jeden lub więcej `Transformer`ów, a ostatni krok to musi być `Estimator` lub innymi słowy model (np. `DecisionTreeRegressor` albo `RandomForestRegressor`).\n",
    "\n",
    "Przykłady transformerów:\n",
    "- [Label Encoder](https://bit.ly/3fobOst) - przypisanie ID dla każdej zmiennej kategorialnej,\n",
    "- [Imputer](https://bit.ly/3uU31oX) - zarządzanie brakami danych,\n",
    "- [SelectKBest](https://bit.ly/2RmOcg2) - wybór *k* najlepszych cech.\n",
    "\n",
    "Możemy również zdefiniować własną klasę tak dla `Transformers`, jak i dla `Estimators`. Zrobienie własnej klasy, która transformuje dane w ten czy inny sposób, zdarza się znacznie częściej, niż tworzenie własnego modelu/estimatora.\n",
    "\n",
    "Na początek zróbmy prosty przykład z modelem liniowym. Najpierw tak jak to robiliśmy dotychczas, a potem z użyciem `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:48.171301Z",
     "start_time": "2021-05-16T14:34:48.167383Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_predict(model, X, y):\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred[ y_pred < 0 ] =0\n",
    "    return rmsle(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:48.213190Z",
     "start_time": "2021-05-16T14:34:48.205551Z"
    }
   },
   "outputs": [],
   "source": [
    "feats = df_train.select_dtypes(include=[np.int64, np.float64]).columns\n",
    "feats = feats[ (feats != 'count') & (feats != 'registered') & (feats != 'casual') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:48.281534Z",
     "start_time": "2021-05-16T14:34:48.249955Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2987262749693944"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_train[feats].values\n",
    "y = df_train['count'].values\n",
    "\n",
    "scalr = MinMaxScaler()\n",
    "X = scalr.fit_transform(X)\n",
    "model = LinearRegression()\n",
    "\n",
    "train_and_predict(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "To samo co wyżej tylko z `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:48.379213Z",
     "start_time": "2021-05-16T14:34:48.321180Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2987262749693944"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_train[feats].values\n",
    "y = df_train['count'].values\n",
    "\n",
    "pipeline = Pipeline( [\n",
    "    ('normalize', MinMaxScaler()),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "train_and_predict(pipeline, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Również możemy dodać kod, który uzupełnia brakujące elementy używając `SimpleImputer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:48.569330Z",
     "start_time": "2021-05-16T14:34:48.470398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2987262749693944"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline( [\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('normalize', MinMaxScaler()),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "train_and_predict(pipeline, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwróć uwagę, że teraz uruchamiamy `fit` i `predict` na `Pipeline` i tam w środku odbywa się cały łańcuch zmian. \n",
    "\n",
    "Idziemy dalej. Spróbujmy np. wybrać cechy, używając `VarianceThreshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:48.740720Z",
     "start_time": "2021-05-16T14:34:48.635629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.273889746042342"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline( [\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('normalize', MinMaxScaler()),\n",
    "    ('variance-treshold', VarianceThreshold(threshold=0.05)),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "train_and_predict(pipeline, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Również możemy użyć rekurencyjnego wybierania cech (pamiętaj, że to jest bardzo efektywna, ale wolna metoda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:48.906754Z",
     "start_time": "2021-05-16T14:34:48.835069Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.272635499068297"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline( [\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('normalize', MinMaxScaler()),\n",
    "    ('variance-treshold', VarianceThreshold(threshold=0.05)),\n",
    "    ('RFE', RFE(LinearRegression())),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "train_and_predict(pipeline, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To wrzućmy teraz bardziej złożony model np. `DecisionTreeRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:49.051979Z",
     "start_time": "2021-05-16T14:34:48.944052Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3323608160094821"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline( [\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('normalize', MinMaxScaler()),\n",
    "    ('variance-treshold', VarianceThreshold(threshold=0.05)),\n",
    "    ('model', DecisionTreeRegressor(max_depth=10))\n",
    "])\n",
    "\n",
    "train_and_predict(pipeline, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A teraz `RandomForestRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:49.299438Z",
     "start_time": "2021-05-16T14:34:49.080856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3251482035054004"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline( [\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('normalize', MinMaxScaler()),\n",
    "    ('variance-treshold', VarianceThreshold(threshold=0.05)),\n",
    "    ('model', RandomForestRegressor(max_depth=10, n_estimators=10))\n",
    "])\n",
    "\n",
    "train_and_predict(pipeline, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swoją drogą, masz również możliwość dostania się do każdego kroku `Pipeline` np. do modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:49.337050Z",
     "start_time": "2021-05-16T14:34:49.330196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=10, n_estimators=10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.named_steps['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spróbujmy napisać własny `Transformer` (nie musisz rozumieć poniższego kodu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:49.368515Z",
     "start_time": "2021-05-16T14:34:49.364320Z"
    }
   },
   "outputs": [],
   "source": [
    "class SelectFeature(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, feature):\n",
    "        self.feature = feature\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data):\n",
    "        return data[self.feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Można pójść dalej i zrobić własny ogólny `Transformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:49.399773Z",
     "start_time": "2021-05-16T14:34:49.395361Z"
    }
   },
   "outputs": [],
   "source": [
    "class DFTransform(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, func, copy=False):\n",
    "        self.func = func\n",
    "        self.copy = copy\n",
    "        \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ = X if not self.copy else X.copy()\n",
    "        return self.func(X_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiujemy funkcje, które wykorzystamy w naszym `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:49.441207Z",
     "start_time": "2021-05-16T14:34:49.435513Z"
    }
   },
   "outputs": [],
   "source": [
    "def _remove_target_vars(df):\n",
    "    feats = df.columns\n",
    "    feats = feats[ (feats != 'count') & (feats != 'registered') & (feats != 'casual') ]\n",
    "    \n",
    "    return df[feats]\n",
    "\n",
    "def _parse_date(df, feat):\n",
    "    df[feat] = pd.to_datetime(df[feat])\n",
    "    return df\n",
    "\n",
    "def _extract_dt(df, feat):\n",
    "    \n",
    "    df['hour'] = df[feat].dt.hour\n",
    "    df['day'] = df[feat].dt.day\n",
    "    df['month'] = df[feat].dt.month\n",
    "    df['year'] = df[feat].dt.year\n",
    "    df['dayofweek'] = df[feat].dt.dayofweek\n",
    "    df['weekofyear'] = df[feat].dt.weekofyear\n",
    "    df['weekend'] = df.dayofweek.map(lambda x: int(x in [5,6]) )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tworzymy `Pipeline` używając naszych funkcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:49.476988Z",
     "start_time": "2021-05-16T14:34:49.471787Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('fill_na', DFTransform(lambda df: df.fillna(-1), copy=True) ),\n",
    "    ('remove_target_var', DFTransform(_remove_target_vars) ),\n",
    "    \n",
    "    ('parse_date',  DFTransform(lambda df: _parse_date(df, 'datetime')) ),\n",
    "    ('extract_dt', DFTransform(lambda df: _extract_dt(df, 'datetime')) ),\n",
    "    \n",
    "    ('numerical', Pipeline([\n",
    "       ('select', DFTransform(lambda df: df.select_dtypes(include=['int'])) ), \n",
    "    ])),\n",
    "    \n",
    "    ('model', RandomForestRegressor(max_depth=10, n_estimators=10, random_state=2019) )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz już dochodzimy do sedna, żeby zobaczyć wartość takiej \"układanki\". \n",
    "\n",
    "Wczytujemy zbiór treningowy, który posiada odpowiedź i zbiór testowy, który odpowiedzi nie ma. Przetwarzanie dla obu zbiorów będzie identyczne (zbiór testowy wprawdzie nie zawiera zmiennych docelowych, więc wykonanie na nim operacji usuwania tych zmiennych jest niepotrzebne, ale to nie przeszkadza, bo zachowujemy spójność `Pipeline`). \n",
    "\n",
    "Zobacz, jak łatwo wygląda teraz predykcja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:34:49.923907Z",
     "start_time": "2021-05-16T14:34:49.508819Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/00/3f1t2rhd45l6fvmbkz20ptgh00dr78/T/ipykernel_9436/3358762817.py:18: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n",
      "  df['weekofyear'] = df[feat].dt.weekofyear\n",
      "/var/folders/00/3f1t2rhd45l6fvmbkz20ptgh00dr78/T/ipykernel_9436/3358762817.py:18: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n",
      "  df['weekofyear'] = df[feat].dt.weekofyear\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 17.14978261,   6.60303533,   6.25903439, ..., 128.37424242,\n",
       "       107.08570707,  74.84654611])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_hdf('../input/train_bike.h5')\n",
    "df_test = pd.read_hdf('../input/test_bike.h5')\n",
    "\n",
    "pipeline.fit(df_train, y)\n",
    "y_pred = pipeline.predict(df_test)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Swoją drogą*, jeśli chodzi *serializację*, to teraz możesz zapisać na dysku całą logikę `Pipeline`. Mam nadzieje, że to już jest dla Ciebie oczywiste, jaką to daje przewagę w porównaniu do *serializacji* tylko \"gołego\" modelu. Pamiętaj tylko, że standardowy `pickle` nie poradzi sobie z `lambdami` (funkcje anonimowe), ale to już było omówione w poprzedniej lekcji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przydatne linki:\n",
    "1. [Deploying Machine Learning using sklearn pipelines](https://bit.ly/3uRHk93)\n",
    "2. [Why you should use scikit-learn's Pipeline object](https://bit.ly/3olRb4h)\n",
    "3. [Integrating Pandas and scikit-learn with pipelines](https://bit.ly/3eO53kG)\n",
    "4. [Pandas, Pipelines, and Custom Transformers](https://bit.ly/3hvjAnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umapkernel",
   "language": "python",
   "name": "umapkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
