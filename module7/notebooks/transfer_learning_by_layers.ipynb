{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "\n",
    "### Celem jest zapoznanie się z dostępnymi modelami (architekturami).\n",
    "\n",
    "### Modele\n",
    "Jak widać Keras ma trochę dostępnych modeli. Jeśli chcesz poczytać więcej o każdym z nich, poniżej są linki:\n",
    "\n",
    "- [ResNet50: Deep Residual Learning for Image Recognition](https://bit.ly/3b61MLt)\n",
    "- [VGG16: Very Deep Convolutional Networks for Large-Scale Image Recognition](https://bit.ly/3xYwjVs)\n",
    "- [VGG19: Very Deep Convolutional Networks for Large-Scale Image Recognition](https://bit.ly/3xYwjVs)\n",
    "- [InceptionResNetV2: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://bit.ly/3heUn07)\n",
    "- [InceptionV3: Rethinking the Inception Architecture for Computer Vision](https://bit.ly/3o0LJnw)\n",
    "- [MobileNet: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://bit.ly/2RA1PrU)\n",
    "- [Xception: Deep Learning with Depthwise Separable Convolutions](https://bit.ly/3vJpvJ8)\n",
    "\n",
    "Weźmiemy na początek jeden z prostszych, ale dość popularny - **VGG16**.\n",
    "\n",
    "*Swoją drogą*, wczytując model, mamy flagę `include_top=False`, która oznacza, że obcinamy warstwę klasyfikatora (czyli `fully-connected layer`). Na początek wczytamy całość (razem z `fully-connected layers`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-09T16:18:10.129751Z",
     "start_time": "2021-05-09T16:18:08.124463Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.densenet import DenseNet121, DenseNet169, DenseNet201\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-09T16:18:14.371204Z",
     "start_time": "2021-05-09T16:18:10.171542Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-11 15:43:35.968464: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 23s 0us/step\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaki widać VGG16 ma ponad 138M parametrów (bez `fully-connected` ma 14M i trochę, czyli 10 razy mniej). Domyślnie wszystkie warstwy są \"odmrożone\" (zwróć uwagę na ostatnią linię poprzedniej komórki: `Non-trainable params: 0`).\n",
    "\n",
    "Do warstw można dostać się w taki sposób: `model.layers`. Każda warstwa ma atrybut `trainable`, który zwraca `True` lub `False`. Sprawdźmy to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-09T16:18:14.412455Z",
     "start_time": "2021-05-09T16:18:14.401764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 True\n",
      "1 True\n",
      "2 True\n",
      "3 True\n",
      "4 True\n",
      "5 True\n",
      "6 True\n",
      "7 True\n",
      "8 True\n",
      "9 True\n",
      "10 True\n",
      "11 True\n",
      "12 True\n",
      "13 True\n",
      "14 True\n",
      "15 True\n",
      "16 True\n",
      "17 True\n",
      "18 True\n",
      "19 True\n",
      "20 True\n",
      "21 True\n",
      "22 True\n"
     ]
    }
   ],
   "source": [
    "for it, layer in enumerate(model.layers):\n",
    "    print(it, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierwsza warstwa już jest ustawiona na `False`, bo jest to `InputLayer`. Załóżmy, że chcemy trenować (ang. *fine tuning*) tylko ostatni (5. blok), który składa się z czterech warstw (niskopoziomowych):\n",
    "\n",
    "```\n",
    "block5_conv1 (Conv2D)     (None, None, None, 512)   2359808   \n",
    "block5_conv2 (Conv2D)     (None, None, None, 512)   2359808   \n",
    "block5_conv3 (Conv2D)     (None, None, None, 512)   2359808   \n",
    "block5_pool (MaxPool2D)   (None, None, None, 512)   0 \n",
    "```\n",
    "\n",
    "\n",
    "W takiej sytuacji można dla reszty warstw ustawić parametr `trainable` na `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-09T16:18:14.447607Z",
     "start_time": "2021-05-09T16:18:14.443001Z"
    }
   },
   "outputs": [],
   "source": [
    "for it, layer in enumerate(model.layers[:-8]):\n",
    "    layer.trainable = False\n",
    "for it, layer in enumerate(model.layers[-4:]):\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzamy całość i potem jeszcze `summary` (zwracamy uwagę na ostatnią linię - `Non-trainable params`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-09T16:18:14.491607Z",
     "start_time": "2021-05-09T16:18:14.478023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1 False\n",
      "1 block1_conv1 False\n",
      "2 block1_conv2 False\n",
      "3 block1_pool False\n",
      "4 block2_conv1 False\n",
      "5 block2_conv2 False\n",
      "6 block2_pool False\n",
      "7 block3_conv1 False\n",
      "8 block3_conv2 False\n",
      "9 block3_conv3 False\n",
      "10 block3_pool False\n",
      "11 block4_conv1 False\n",
      "12 block4_conv2 False\n",
      "13 block4_conv3 False\n",
      "14 block4_pool False\n",
      "15 block5_conv1 True\n",
      "16 block5_conv2 True\n",
      "17 block5_conv3 True\n",
      "18 block5_pool True\n",
      "19 flatten False\n",
      "20 fc1 False\n",
      "21 fc2 False\n",
      "22 predictions False\n"
     ]
    }
   ],
   "source": [
    "for it, layer in enumerate(model.layers):\n",
    "    print(it, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-09T16:18:14.531374Z",
     "start_time": "2021-05-09T16:18:14.521166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 7,079,424\n",
      "Non-trainable params: 131,278,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-09T16:18:15.096208Z",
     "start_time": "2021-05-09T16:18:14.562712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 4s 0us/step\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, None, None, 3)]   0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przypadki zadań \n",
    "W dużym uproszczeniu wszystkie zadania w Deep Learning może klasyfikować wg dwóch wymiarów:\n",
    "![](../images/four_cases.png)\n",
    "\n",
    "A teraz przejdziemy po kolei po każdym przypadku."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Danych jest sporo i zadanie bardzo się różni.\n",
    "\n",
    "W takiej sytuacji tak naprawdę uczymy naszą sieć od zera (czyli `ImageNet` trafia do tego przypadku, kiedy mamy miliony zdjęć i zupełnie nowe zadanie)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Danych jest dużo, ale zadanie jest podobne.\n",
    "\n",
    "Skoro mamy dużo danych, to możemy uczyć się nawet od zera (przynajmniej nie powinien nam grozić `overfitting`). Z drugiej strony, skoro zadanie jest podobne, to warto wykorzystać już wyliczone wagi. \n",
    "\n",
    "Dlatego strategia jest taka - odmrażamy wszystkie warstwy (`trainable=True`), to jest opcja domyślna. Jedynie jeszcze można przerobić `fully-connected layer` - chyba, że to też nam odpowiada (np. `ImageNet` ma 1000 klas na wyjściu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-09T16:18:15.631833Z",
     "start_time": "2021-05-09T16:18:15.128905Z"
    }
   },
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
    "\n",
    "for layer in base_model.layers[1:]: #input layer ma być na False\n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    \n",
    "    Flatten(), #<= bridge between conv layers and full connected layers\n",
    "    \n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "    \n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzamy, jak to jest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-09T16:18:15.672004Z",
     "start_time": "2021-05-09T16:18:15.663009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg16 True\n",
      "flatten True\n",
      "dense True\n",
      "dropout True\n",
      "dense_1 True\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 1, 1, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 15,250,250\n",
      "Trainable params: 15,250,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwróć uwagę, że VGG16 teraz jest pokazany jako warstwa - jest to dość wygodne, w `summary` widać tylko to, co jest ważne (nasze)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Danych jest mało, ale i zadanie jest podobne.\n",
    "\n",
    "W sytuacji gdy danych jest mało lub wcale ich nie ma, to musimy użyć takiego modelu, jaki jest (właśnie to już zrobiliśmy w punkcie `2`) albo zamrozić wszystkie warstwy konwolucyjne i wyrzucić lub odmrozić `fully-connected layers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-09T16:18:16.194294Z",
     "start_time": "2021-05-09T16:18:15.695676Z"
    }
   },
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
    "\n",
    "for layer in base_model.layers[1:]: #input layer ma być na False\n",
    "    layer.trainable = False #!!!! zwróć uwagę, zamrażamy wszystkie warstwy\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    \n",
    "    Flatten(), #<= bridge between conv layers and full connected layers\n",
    "    \n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "    \n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzamy, jak to jest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-09T16:18:16.225056Z",
     "start_time": "2021-05-09T16:18:16.217659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg16 True\n",
      "flatten_1 True\n",
      "dense_2 True\n",
      "dropout_1 True\n",
      "dense_3 True\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 1, 1, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 15,250,250\n",
      "Trainable params: 535,562\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W drugim przypadku było `Trainable params: 15,250,250`, a w tym przypadku jest: `Trainable params: 535,562`. Trenujemy tylko `fully-connected layer` i nawet mając mało danych jest szansa, że nie będziemy \"overfitować\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Danych jest mało i zadanie (bardzo) się różni.\n",
    "To jest najgorszy przypadek i niestety dość częsty :) Trzeba sobie jakoś z tym radzić. W tej sytuacji odcięcie tylko `full-connected layer` to raczej za mało - trzeba odmrozić coś więcej (kilka lub więcej warstw konwolucyjnych). Natomiast możemy tutaj zastosować dość sprytne zagranie, które zwykle sprawdza się w praktyce.\n",
    "\n",
    "Na początek zamrażamy wszystkie warstwy konwolucyjne i trenujemy tylko `fully-connected layer`, a następnie odmrażamy wszystko, tylko trenujemy z mniejszym krokiem.\n",
    "\n",
    "### Faza pierwsza\n",
    "Zamrażamy wszystkie warstwy konwolucyjne i trenujemy tylko `fully-connected layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-09T16:18:16.775154Z",
     "start_time": "2021-05-09T16:18:16.257044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg16 True\n",
      "flatten_2 True\n",
      "dense_4 True\n",
      "dropout_2 True\n",
      "dense_5 True\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 1, 1, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 15,250,250\n",
      "Trainable params: 535,562\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
    "\n",
    "for layer in base_model.layers[1:]: #input layer ma być na False\n",
    "    layer.trainable = False #!!!! zwróć uwagę, zamrażamy wszystkie warstwy\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    \n",
    "    Flatten(), #<= bridge between conv layers and full connected layers\n",
    "    \n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "    \n",
    "])\n",
    "\n",
    "optimizer = Adam(0.001, decay=0.0003) #wartości są przykładowe, zwróć uwagę tylko na rząd\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faza druga\n",
    "\n",
    "Odmrażamy wszystkie lub część warstw z VGG. Swoją drogą, żeby się tam dostać trzeba odwołać się do pierwszej warstwy `model.layers[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-09T16:18:16.819302Z",
     "start_time": "2021-05-09T16:18:16.800418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_5 True\n",
      "block1_conv1 False\n",
      "block1_conv2 False\n",
      "block1_pool False\n",
      "block2_conv1 False\n",
      "block2_conv2 False\n",
      "block2_pool False\n",
      "block3_conv1 False\n",
      "block3_conv2 False\n",
      "block3_conv3 False\n",
      "block3_pool True\n",
      "block4_conv1 True\n",
      "block4_conv2 True\n",
      "block4_conv3 True\n",
      "block4_pool True\n",
      "block5_conv1 True\n",
      "block5_conv2 True\n",
      "block5_conv3 True\n",
      "block5_pool True\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 1, 1, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 15,250,250\n",
      "Trainable params: 13,514,762\n",
      "Non-trainable params: 1,735,488\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers[0].layers[10:]: \n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = Adam(0.0001, decay=0.00000001) #wartości są przykładowe, zwróć uwagę tylko na rząd\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "for layer in model.layers[0].layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przydatne linki:\n",
    "1. [CS231n: Transfer Learning](https://bit.ly/2RDIBBy)\n",
    "2. [Learning Keras by Implementing the VGG Network From Scratch](https://bit.ly/2Q4uZPG)\n",
    "3. [Experiments on different loss configurations for style transfer](https://bit.ly/3vQP7DU)\n",
    "4. [Transfer Learning using Keras](https://bit.ly/3nYe0ed)\n",
    "5. [A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning)](https://bit.ly/3b9zAYb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umapkernel",
   "language": "python",
   "name": "umapkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
